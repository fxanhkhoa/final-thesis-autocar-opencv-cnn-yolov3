{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train_detectline_trafficsign.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"IliJSqUVvA-Y","colab_type":"code","outputId":"1160728b-5699-4db4-9346-42db988aaaf0","executionInfo":{"status":"ok","timestamp":1574681923165,"user_tz":-420,"elapsed":1510,"user":{"displayName":"KHOA BÙI ANH","photoUrl":"","userId":"14113044718160609154"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dmx48SNg0gya","colab_type":"code","outputId":"6055d2f8-80ef-48c0-c1aa-e4cf7cdbb813","executionInfo":{"status":"ok","timestamp":1574681928553,"user_tz":-420,"elapsed":4170,"user":{"displayName":"KHOA BÙI ANH","photoUrl":"","userId":"14113044718160609154"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%cd /content/drive/My\\ Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3\n","!pwd\n","\n","################ PLEASE DETELE Train.pkl FILE BEFORE TRAINING ##################"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JcJPE7rZ4QN3","colab_type":"code","colab":{}},"source":["import random\n","import argparse\n","import numpy as np\n","\n","from voc import parse_voc_annotation\n","import json"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrh3kSOxL7GW","colab_type":"code","colab":{}},"source":["def IOU(ann, centroids):\n","    w, h = ann\n","    similarities = []\n","\n","    for centroid in centroids:\n","        c_w, c_h = centroid\n","\n","        if c_w >= w and c_h >= h:\n","            similarity = w*h/(c_w*c_h)\n","        elif c_w >= w and c_h <= h:\n","            similarity = w*c_h/(w*h + (c_w-w)*c_h)\n","        elif c_w <= w and c_h >= h:\n","            similarity = c_w*h/(w*h + c_w*(c_h-h))\n","        else: #means both w,h are bigger than c_w and c_h respectively\n","            similarity = (c_w*c_h)/(w*h)\n","        similarities.append(similarity) # will become (k,) shape\n","\n","    return np.array(similarities)\n","\n","def avg_IOU(anns, centroids):\n","    n,d = anns.shape\n","    sum = 0.\n","\n","    for i in range(anns.shape[0]):\n","        sum+= max(IOU(anns[i], centroids))\n","\n","    return sum/n\n","\n","def print_anchors(centroids):\n","    out_string = ''\n","\n","    anchors = centroids.copy()\n","\n","    widths = anchors[:, 0]\n","    sorted_indices = np.argsort(widths)\n","\n","    r = \"anchors: [\"\n","    for i in sorted_indices:\n","        out_string += str(int(anchors[i,0]*416)) + ',' + str(int(anchors[i,1]*416)) + ', '\n","            \n","    print(out_string[:-2])\n","\n","def run_kmeans(ann_dims, anchor_num):\n","    ann_num = ann_dims.shape[0]\n","    iterations = 0\n","    prev_assignments = np.ones(ann_num)*(-1)\n","    iteration = 0\n","    old_distances = np.zeros((ann_num, anchor_num))\n","\n","    indices = [random.randrange(ann_dims.shape[0]) for i in range(anchor_num)]\n","    centroids = ann_dims[indices]\n","    anchor_dim = ann_dims.shape[1]\n","\n","    while True:\n","        distances = []\n","        iteration += 1\n","        for i in range(ann_num):\n","            d = 1 - IOU(ann_dims[i], centroids)\n","            distances.append(d)\n","        distances = np.array(distances) # distances.shape = (ann_num, anchor_num)\n","\n","        print(\"iteration {}: dists = {}\".format(iteration, np.sum(np.abs(old_distances-distances))))\n","\n","        #assign samples to centroids\n","        assignments = np.argmin(distances,axis=1)\n","\n","        if (assignments == prev_assignments).all() :\n","            return centroids\n","\n","        #calculate new centroids\n","        centroid_sums=np.zeros((anchor_num, anchor_dim), np.float)\n","        for i in range(ann_num):\n","            centroid_sums[assignments[i]]+=ann_dims[i]\n","        for j in range(anchor_num):\n","            centroids[j] = centroid_sums[j]/(np.sum(assignments==j) + 1e-6)\n","\n","        prev_assignments = assignments.copy()\n","        old_distances = distances.copy()\n","\n","def _main_():\n","    config_path = \"configWithText.json\"\n","    num_anchors = 9\n","\n","    with open(config_path) as config_buffer:\n","        config = json.loads(config_buffer.read())\n","\n","    train_imgs, train_labels = parse_voc_annotation(\n","        config['train']['train_annot_folder'],\n","        config['train']['train_image_folder'],\n","        config['train']['cache_name'],\n","        config['model']['labels']\n","    )\n","\n","    # run k_mean to find the anchors\n","    annotation_dims = []\n","    for image in train_imgs:\n","        print(image['filename'])\n","        for obj in image['object']:\n","            relative_w = (float(obj['xmax']) - float(obj['xmin']))/image['width']\n","            relatice_h = (float(obj[\"ymax\"]) - float(obj['ymin']))/image['height']\n","            annotation_dims.append(tuple(map(float, (relative_w,relatice_h))))\n","\n","    annotation_dims = np.array(annotation_dims)\n","    centroids = run_kmeans(annotation_dims, num_anchors)\n","\n","    # write anchors to file\n","    print('\\naverage IOU for', num_anchors, 'anchors:', '%0.2f' % avg_IOU(annotation_dims, centroids))\n","    print_anchors(centroids)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yV6NrHgaMfaN","colab_type":"code","outputId":"0f9b476f-1408-4943-b970-b611b656d711","executionInfo":{"status":"ok","timestamp":1574679825383,"user_tz":-420,"elapsed":10286,"user":{"displayName":"KHOA BÙI ANH","photoUrl":"","userId":"14113044718160609154"}},"colab":{"base_uri":"https://localhost:8080/","height":578}},"source":["_main_()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0001.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0006.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0007.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0008.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0009.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0010.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0056.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0057.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0058.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0059.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0060.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0061.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0067.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0068.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0069.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0070.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0071.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0072.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0073.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0074.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0075.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0076.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0077.png\n","/content/drive/My Drive/AI_COLAB/detect_line_traffic_sign_YOLOv3/data_raw_yolov3-Copy/0078.png\n","iteration 1: dists = 227.73293786887623\n","iteration 2: dists = 38.65788334610089\n","iteration 3: dists = 2.085883908976322\n","iteration 4: dists = 3.1801535603022026\n","iteration 5: dists = 4.063390330326003\n","iteration 6: dists = 1.7602233610182088\n","\n","average IOU for 9 anchors: 0.91\n","0,0, 0,0, 22,29, 34,44, 46,58, 62,82, 89,116, 414,93, 414,79\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4jjMOo31PRCS","colab_type":"code","colab":{}},"source":["import argparse\n","import os\n","import numpy as np\n","import json\n","from voc import parse_voc_annotation\n","from yolo import create_yolov3_model, dummy_loss\n","from generator import BatchGenerator\n","from utils.utils import normalize, evaluate, makedirs\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from keras.optimizers import Adam\n","from callbacks import CustomModelCheckpoint, CustomTensorBoard\n","from utils.multi_gpu_model import multi_gpu_model\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import keras\n","from keras.models import load_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMI5wEcrPlxS","colab_type":"code","colab":{}},"source":["def create_training_instances(\n","    train_annot_folder,\n","    train_image_folder,\n","    train_cache,\n","    valid_annot_folder,\n","    valid_image_folder,\n","    valid_cache,\n","    labels,\n","):\n","    # parse annotations of the training set\n","    train_ints, train_labels = parse_voc_annotation(train_annot_folder, train_image_folder, train_cache, labels)\n","\n","    # parse annotations of the validation set, if any, otherwise split the training set\n","    if os.path.exists(valid_annot_folder):\n","        valid_ints, valid_labels = parse_voc_annotation(valid_annot_folder, valid_image_folder, valid_cache, labels)\n","    else:\n","        print(\"valid_annot_folder not exists. Spliting the trainining set.\")\n","\n","        train_valid_split = int(0.8*len(train_ints))\n","        np.random.seed(0)\n","        np.random.shuffle(train_ints)\n","        np.random.seed()\n","\n","        valid_ints = train_ints[train_valid_split:]\n","        train_ints = train_ints[:train_valid_split]\n","\n","    # compare the seen labels with the given labels in config.json\n","    if len(labels) > 0:\n","        overlap_labels = set(labels).intersection(set(train_labels.keys()))\n","\n","        print('Seen labels: \\t'  + str(train_labels) + '\\n')\n","        print('Given labels: \\t' + str(labels))\n","\n","        # return None, None, None if some given label is not in the dataset\n","        if len(overlap_labels) < len(labels):\n","            print('Some labels have no annotations! Please revise the list of labels in the config.json.')\n","            return None, None, None\n","    else:\n","        print('No labels are provided. Train on all seen labels.')\n","        print(train_labels)\n","        labels = train_labels.keys()\n","\n","    max_box_per_image = max([len(inst['object']) for inst in (train_ints + valid_ints)])\n","\n","    return train_ints, valid_ints, sorted(labels), max_box_per_image\n","\n","def create_callbacks(saved_weights_name, tensorboard_logs, model_to_save):\n","    makedirs(tensorboard_logs)\n","    \n","    early_stop = EarlyStopping(\n","        monitor     = 'loss', \n","        min_delta   = 0.01, \n","        patience    = 5, \n","        mode        = 'min', \n","        verbose     = 1\n","    )\n","    checkpoint = CustomModelCheckpoint(\n","        model_to_save   = model_to_save,\n","        filepath        = saved_weights_name + '{epoch:02d}.h5', \n","        monitor         = 'loss', \n","        verbose         = 1, \n","        save_best_only  = True, \n","        mode            = 'min', \n","        period          = 1\n","    )\n","    reduce_on_plateau = ReduceLROnPlateau(\n","        monitor  = 'loss',\n","        factor   = 0.1,\n","        patience = 2,\n","        verbose  = 1,\n","        mode     = 'min',\n","        epsilon  = 0.01,\n","        cooldown = 0,\n","        min_lr   = 0\n","    )\n","    tensorboard = CustomTensorBoard(\n","        log_dir                = tensorboard_logs,\n","        write_graph            = True,\n","        write_images           = True,\n","    )    \n","    return [early_stop, checkpoint, reduce_on_plateau, tensorboard]\n","\n","def create_model(\n","    nb_class, \n","    anchors, \n","    max_box_per_image, \n","    max_grid, batch_size, \n","    warmup_batches, \n","    ignore_thresh, \n","    multi_gpu, \n","    saved_weights_name, \n","    lr,\n","    grid_scales,\n","    obj_scale,\n","    noobj_scale,\n","    xywh_scale,\n","    class_scale  \n","):\n","    if multi_gpu > 1:\n","        with tf.device('/cpu:0'):\n","            template_model, infer_model = create_yolov3_model(\n","                nb_class            = nb_class, \n","                anchors             = anchors, \n","                max_box_per_image   = max_box_per_image, \n","                max_grid            = max_grid, \n","                batch_size          = batch_size//multi_gpu, \n","                warmup_batches      = warmup_batches,\n","                ignore_thresh       = ignore_thresh,\n","                grid_scales         = grid_scales,\n","                obj_scale           = obj_scale,\n","                noobj_scale         = noobj_scale,\n","                xywh_scale          = xywh_scale,\n","                class_scale         = class_scale\n","            )\n","    else:\n","        template_model, infer_model = create_yolov3_model(\n","            nb_class            = nb_class, \n","            anchors             = anchors, \n","            max_box_per_image   = max_box_per_image, \n","            max_grid            = max_grid, \n","            batch_size          = batch_size, \n","            warmup_batches      = warmup_batches,\n","            ignore_thresh       = ignore_thresh,\n","            grid_scales         = grid_scales,\n","            obj_scale           = obj_scale,\n","            noobj_scale         = noobj_scale,\n","            xywh_scale          = xywh_scale,\n","            class_scale         = class_scale\n","        )  \n","\n","    # load the pretrained weight if exists, otherwise load the backend weight only\n","    if os.path.exists(saved_weights_name): \n","        print(\"\\nLoading pretrained weights.\\n\")\n","        template_model.load_weights(saved_weights_name)\n","    else:\n","        template_model.load_weights(\"backend.h5\", by_name=True)       \n","\n","    if multi_gpu > 1:\n","        train_model = multi_gpu_model(template_model, gpus=multi_gpu)\n","    else:\n","        train_model = template_model      \n","\n","    optimizer = Adam(lr=lr, clipnorm=0.001)\n","    train_model.compile(loss=dummy_loss, optimizer=optimizer)             \n","\n","    return train_model, infer_model\n","\n","def _main_():\n","    config_path = \"configWithText.json\"\n","\n","    with open(config_path) as config_buffer:    \n","        config = json.loads(config_buffer.read())\n","\n","    ###############################\n","    #   Parse the annotations \n","    ###############################\n","    train_ints, valid_ints, labels, max_box_per_image = create_training_instances(\n","        config['train']['train_annot_folder'],\n","        config['train']['train_image_folder'],\n","        config['train']['cache_name'],\n","        config['valid']['valid_annot_folder'],\n","        config['valid']['valid_image_folder'],\n","        config['valid']['cache_name'],\n","        config['model']['labels']\n","    )\n","    print('\\nTraining on: \\t' + str(labels) + '\\n')\n","\n","    ###############################\n","    #   Create the generators \n","    ###############################    \n","    train_generator = BatchGenerator(\n","        instances           = train_ints, \n","        anchors             = config['model']['anchors'],   \n","        labels              = labels,        \n","        downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n","        max_box_per_image   = max_box_per_image,\n","        batch_size          = config['train']['batch_size'],\n","        min_net_size        = config['model']['min_input_size'],\n","        max_net_size        = config['model']['max_input_size'],   \n","        shuffle             = True, \n","        jitter              = 0.3, \n","        norm                = normalize\n","    )\n","    \n","    valid_generator = BatchGenerator(\n","        instances           = valid_ints, \n","        anchors             = config['model']['anchors'],   \n","        labels              = labels,        \n","        downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n","        max_box_per_image   = max_box_per_image,\n","        batch_size          = config['train']['batch_size'],\n","        min_net_size        = config['model']['min_input_size'],\n","        max_net_size        = config['model']['max_input_size'],   \n","        shuffle             = True, \n","        jitter              = 0.0, \n","        norm                = normalize\n","    )\n","\n","    ###############################\n","    #   Create the model \n","    ###############################\n","    if os.path.exists(config['train']['saved_weights_name']): \n","        config['train']['warmup_epochs'] = 0\n","    warmup_batches = config['train']['warmup_epochs'] * (config['train']['train_times']*len(train_generator))   \n","\n","    #os.environ['CUDA_VISIBLE_DEVICES'] = config['train']['gpus']\n","    #multi_gpu = len(config['train']['gpus'].split(','))\n","    multi_gpu = 1\n","    print(\"PASS\")\n","\n","    train_model, infer_model = create_model(\n","        nb_class            = len(labels), \n","        anchors             = config['model']['anchors'], \n","        max_box_per_image   = max_box_per_image, \n","        max_grid            = [config['model']['max_input_size'], config['model']['max_input_size']], \n","        batch_size          = config['train']['batch_size'], \n","        warmup_batches      = warmup_batches,\n","        ignore_thresh       = config['train']['ignore_thresh'],\n","        multi_gpu           = multi_gpu,\n","        saved_weights_name  = config['train']['saved_weights_name'],\n","        lr                  = config['train']['learning_rate'],\n","        grid_scales         = config['train']['grid_scales'],\n","        obj_scale           = config['train']['obj_scale'],\n","        noobj_scale         = config['train']['noobj_scale'],\n","        xywh_scale          = config['train']['xywh_scale'],\n","        class_scale         = config['train']['class_scale'],\n","    )\n","    print(\"PASS2\")\n","\n","    ###############################\n","    #   Kick off the training\n","    ###############################\n","    callbacks = create_callbacks(config['train']['saved_weights_name'], config['train']['tensorboard_dir'], infer_model)\n","  \n","    print(\"PASS3\")\n","    \n","    train_model.fit_generator(\n","        generator        = train_generator, \n","        steps_per_epoch  = len(train_generator) * config['train']['train_times'], \n","        epochs           = config['train']['nb_epochs'] + config['train']['warmup_epochs'], \n","        verbose          = 2 if config['train']['debug'] else 1,\n","        callbacks        = callbacks, \n","        workers          = 4,\n","        max_queue_size   = 8\n","    )\n","    \n","    print(\"PASS4\")\n","    # make a GPU version of infer_model for evaluation\n","    if multi_gpu > 1:\n","        infer_model = load_model(config['train']['saved_weights_name'])\n","\n","    ###############################\n","    #   Run the evaluation\n","    ###############################   \n","    # compute mAP for all the classes\n","    average_precisions = evaluate(infer_model, valid_generator)\n","\n","    # print the score\n","    for label, average_precision in average_precisions.items():\n","        print(labels[label] + ': {:.4f}'.format(average_precision))\n","    print('mAP: {:.4f}'.format(sum(average_precisions.values()) / len(average_precisions)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QM6hHJrRPubv","colab_type":"code","outputId":"c27ae68f-8f65-4f08-cf58-3a8765805cbe","executionInfo":{"status":"ok","timestamp":1574682754116,"user_tz":-420,"elapsed":775483,"user":{"displayName":"KHOA BÙI ANH","photoUrl":"","userId":"14113044718160609154"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["_main_()"],"execution_count":23,"outputs":[{"output_type":"stream","text":["valid_annot_folder not exists. Spliting the trainining set.\n","Seen labels: \t{'NoLeft': 24, '0': 15, '-5': 9}\n","\n","Given labels: \t['-5', 'NoLeft', '0']\n","\n","Training on: \t['-5', '0', 'NoLeft']\n","\n","PASS\n","\n","Loading pretrained weights.\n","\n","PASS2\n","PASS3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n","  warnings.warn('`epsilon` argument is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/20\n","resizing:  416 416\n","resizing:  384 384\n","resizing:  384 384\n","resizing:  352 352\n","resizing:  448 448\n","resizing:  384 384\n","resizing:  448 448\n","resizing:  288 288\n","resizing:  416 416\n"," - 55s - loss: 5.3367 - yolo_layer_4_loss: 2.2996 - yolo_layer_5_loss: 1.8335 - yolo_layer_6_loss: 1.2036\n","\n","Epoch 00001: loss improved from inf to 5.33673, saving model to autocar_weight.h513.h501.h5\n","Epoch 2/20\n","resizing:  288 288\n","resizing:  416 416\n","resizing:  384 384\n","resizing:  352 352\n","resizing:  448 448\n","resizing:  352 352\n","resizing:  352 352\n","resizing:  320 320\n"," - 29s - loss: 5.8563 - yolo_layer_4_loss: 2.4521 - yolo_layer_5_loss: 1.9202 - yolo_layer_6_loss: 1.4840\n","\n","Epoch 00002: loss did not improve from 5.33673\n","Epoch 3/20\n","resizing:  320 320\n","resizing:  448 448\n","resizing:  352 352\n","resizing:  416 416\n","resizing:  416 416\n","resizing:  448 448\n","resizing:  416 416\n","resizing:  352 352\n"," - 31s - loss: 5.3369 - yolo_layer_4_loss: 2.2171 - yolo_layer_5_loss: 1.9888 - yolo_layer_6_loss: 1.1310\n","\n","Epoch 00003: loss did not improve from 5.33673\n","\n","Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n","Epoch 4/20\n","resizing:  384 384\n","resizing:  384 384\n","resizing:  352 352\n","resizing:  352 352\n","resizing:  320 320\n","resizing:  448 448\n","resizing:  320 320\n","resizing:  352 352\n"," - 28s - loss: 4.5850 - yolo_layer_4_loss: 1.9429 - yolo_layer_5_loss: 1.6294 - yolo_layer_6_loss: 1.0127\n","\n","Epoch 00004: loss improved from 5.33673 to 4.58496, saving model to autocar_weight.h513.h504.h5\n","Epoch 5/20\n","resizing:  448 448\n","resizing:  416 416\n","resizing:  448 448\n","resizing:  416 416\n","resizing:  416 416\n","resizing:  448 448\n","resizing:  448 448\n","resizing:  288 288\n"," - 32s - loss: 4.3160 - yolo_layer_4_loss: 2.0034 - yolo_layer_5_loss: 1.4223 - yolo_layer_6_loss: 0.8903\n","\n","Epoch 00005: loss improved from 4.58496 to 4.31600, saving model to autocar_weight.h513.h505.h5\n","Epoch 6/20\n","resizing:  416 416\n","resizing:  352 352\n","resizing:  320 320\n","resizing:  352 352\n","resizing:  384 384\n","resizing:  288 288\n","resizing:  448 448\n","resizing:  448 448\n"," - 27s - loss: 4.2081 - yolo_layer_4_loss: 1.6704 - yolo_layer_5_loss: 1.5527 - yolo_layer_6_loss: 0.9851\n","\n","Epoch 00006: loss improved from 4.31600 to 4.20811, saving model to autocar_weight.h513.h506.h5\n","Epoch 7/20\n","resizing:  416 416\n","resizing:  352 352\n","resizing:  352 352\n","resizing:  288 288\n","resizing:  352 352\n","resizing:  416 416\n","resizing:  320 320\n","resizing:  384 384\n"," - 29s - loss: 3.9239 - yolo_layer_4_loss: 1.7689 - yolo_layer_5_loss: 1.3209 - yolo_layer_6_loss: 0.8341\n","\n","Epoch 00007: loss improved from 4.20811 to 3.92386, saving model to autocar_weight.h513.h507.h5\n","Epoch 8/20\n","resizing:  448 448\n","resizing:  416 416\n","resizing:  384 384\n","resizing:  352 352\n","resizing:  448 448\n","resizing:  416 416\n","resizing:  384 384\n","resizing:  352 352\n"," - 31s - loss: 3.7487 - yolo_layer_4_loss: 1.8452 - yolo_layer_5_loss: 1.2087 - yolo_layer_6_loss: 0.6947\n","\n","Epoch 00008: loss improved from 3.92386 to 3.74866, saving model to autocar_weight.h513.h508.h5\n","Epoch 9/20\n","resizing:  384 384\n","resizing:  352 352\n","resizing:  352 352\n","resizing:  416 416\n","resizing:  448 448\n","resizing:  384 384\n","resizing:  448 448\n","resizing:  384 384\n"," - 30s - loss: 3.8568 - yolo_layer_4_loss: 1.6489 - yolo_layer_5_loss: 1.1658 - yolo_layer_6_loss: 1.0421\n","\n","Epoch 00009: loss did not improve from 3.74866\n","Epoch 10/20\n","resizing:  384 384\n","resizing:  416 416\n","resizing:  288 288\n","resizing:  288 288\n","resizing:  320 320\n","resizing:  416 416\n","resizing:  384 384\n","resizing:  320 320\n"," - 27s - loss: 3.9664 - yolo_layer_4_loss: 1.6127 - yolo_layer_5_loss: 1.4726 - yolo_layer_6_loss: 0.8811\n","\n","Epoch 00010: loss did not improve from 3.74866\n","\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n","Epoch 11/20\n","resizing:  320 320\n","resizing:  352 352\n","resizing:  416 416\n","resizing:  416 416\n","resizing:  416 416\n","resizing:  448 448\n","resizing:  448 448\n","resizing:  288 288\n"," - 30s - loss: 3.6810 - yolo_layer_4_loss: 1.8732 - yolo_layer_5_loss: 1.1215 - yolo_layer_6_loss: 0.6864\n","\n","Epoch 00011: loss improved from 3.74866 to 3.68104, saving model to autocar_weight.h513.h511.h5\n","Epoch 12/20\n","resizing:  352 352\n","resizing:  288 288\n","resizing:  416 416\n","resizing:  416 416\n","resizing:  352 352\n","resizing:  320 320\n","resizing:  288 288\n","resizing:  416 416\n"," - 27s - loss: 3.4933 - yolo_layer_4_loss: 1.6831 - yolo_layer_5_loss: 1.0597 - yolo_layer_6_loss: 0.7506\n","\n","Epoch 00012: loss improved from 3.68104 to 3.49331, saving model to autocar_weight.h513.h512.h5\n","Epoch 13/20\n","resizing:  384 384\n","resizing:  352 352\n","resizing:  352 352\n","resizing:  352 352\n","resizing:  416 416\n","resizing:  384 384\n","resizing:  448 448\n","resizing:  288 288\n"," - 30s - loss: 3.8852 - yolo_layer_4_loss: 1.7740 - yolo_layer_5_loss: 1.1899 - yolo_layer_6_loss: 0.9213\n","\n","Epoch 00013: loss did not improve from 3.49331\n","Epoch 14/20\n","resizing:  320 320\n","resizing:  288 288\n","resizing:  448 448\n","resizing:  416 416\n","resizing:  416 416\n","resizing:  320 320\n","resizing:  448 448\n","resizing:  352 352\n"," - 30s - loss: 3.8993 - yolo_layer_4_loss: 1.8202 - yolo_layer_5_loss: 1.1440 - yolo_layer_6_loss: 0.9352\n","\n","Epoch 00014: loss did not improve from 3.49331\n","\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n","Epoch 15/20\n","resizing:  320 320\n","resizing:  416 416\n","resizing:  416 416\n","resizing:  352 352\n","resizing:  352 352\n","resizing:  384 384\n","resizing:  384 384\n","resizing:  288 288\n"," - 29s - loss: 3.9076 - yolo_layer_4_loss: 1.5850 - yolo_layer_5_loss: 1.2286 - yolo_layer_6_loss: 1.0940\n","\n","Epoch 00015: loss did not improve from 3.49331\n","Epoch 16/20\n","resizing:  384 384\n","resizing:  384 384\n","resizing:  352 352\n","resizing:  448 448\n","resizing:  416 416\n","resizing:  448 448\n","resizing:  384 384\n","resizing:  320 320\n"," - 29s - loss: 3.7318 - yolo_layer_4_loss: 1.6881 - yolo_layer_5_loss: 1.1683 - yolo_layer_6_loss: 0.8754\n","\n","Epoch 00016: loss did not improve from 3.49331\n","\n","Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n","Epoch 17/20\n","resizing:  320 320\n","resizing:  384 384\n","resizing:  288 288\n","resizing:  352 352\n","resizing:  448 448\n","resizing:  416 416\n","resizing:  320 320\n","resizing:  416 416\n"," - 27s - loss: 3.7536 - yolo_layer_4_loss: 1.6759 - yolo_layer_5_loss: 1.2183 - yolo_layer_6_loss: 0.8595\n","\n","Epoch 00017: loss did not improve from 3.49331\n","Epoch 00017: early stopping\n","PASS4\n","-5: 0.0000\n","0: 0.9167\n","NoLeft: 1.0000\n","mAP: 0.6389\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k72nMZYaPQ_2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}